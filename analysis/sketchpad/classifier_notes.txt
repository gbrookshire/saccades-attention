
Ridge
- Reduces over-fitting by "shrinking" all model parameters toward 0
	- But they are not exactly zero
- lambda is the shrinkage parameter
	- lambda --> 0: Closer to a standard linear regression
	- Higher lambda corresponds to greater parameter shrinkage
	- In the sklearn function `Ridge`, this is called "alpha"
- Somebody online says it's important to standardize the variables beforehand

LASSO
- Reduces over-fitting by setting all but the most important parameters to 0
	- Also shrinks the parameters that are retained in the model
- Value of lambda determines how many parameters are set to 0
	- lambda --> 0: Closer to a standard linear regression
	- Higher lambda means more parameters are set to zero
	- Some sklearn functions call this parameter `alpha`
- Find the optimal value of lambda by cross-validation
	- Optimal value minimizes the CV error and minimizes model complexity
- Standardize variables beforehand?

Elastic net
- A combination of LASSO and Ridge regression
- alpha determines what the combination is
	- alpha = 0: Ridge
	- alpha = 1: LASSO
	- 0 < alpha < 1: Elastic net regression

Implementation
* sklearn
- Several different functions might be relevant
	- Lasso
		- LassoCV helps select regularization parameter
	- LogisticRegression
		- Some args that might be useful...
		- penalty: l1, l2, elasticnet
		- C: inverse of regularization strength
			- check against inverse of `alpha` in `Lasso`
			- This should have no effect when penalty='none'
			- Smaller values specify stronger regularization
		- solver: can influence speed (and feasibility)
		- multi_class:
			- ovr - binary fit for each label
			- multinomial - predicts specific label
		- max_iter: extend number of iterations to solve
		- l1_ratio: elastic net mixing parameter
		- Choose C and l1_ratio using `LogisticRegressionCV`
	- Look into different ways to score accuracy
		- fit.predict
		- fit.predict_proba
		- fit.score
		- fit.sparsify to get a smaller/faster matrix
* glmnet

Analysis notes
- LogisticRegressionCV
    - Useful to figure out a good regularization parameter C/alpha/lambda
    - multi_class:
        - It works alright with multi_class='multinomial', but not with 'ovr'
            - with 'ovr', it always sets all (or almost all) coefficients to zero
            - and chooses the smallest value of C regardless
        - Can we pick the regularization parameter using 'multinomial',
            but then do our main analyses with 'ovr'?
            - I think this should be fine, since it just chooses important variables
    - scoring: Works much better with 'accuracy' than with 'neg_mean_squared_error'
- LogisticRegression
    - multi_class:
        - ovr: Fit separate binary models for each label
        - multinomial: One model that simultaneously finds each label
    - n_jobs: Instead of parallelizing here, do it in cross_validate
    - Speed comparisons:
        - I ran a quick comparison on one timepoint of Yali's data
            - using 5-fold CV
        - Accuracy scores are (almost) identical
        - liblinear & ovr: Fastest! 0.23 s
        - liblinear & multinomial: not supported
        - saga & multinomial: 41 s
        - saga & ovr: 50 s 
- cross_validate
    - Fits the models with cross-validation
    - scoring: lots of algorithms to choose from
        - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
        - accuracy: Checks for matching labels
        - roc_auc: I'm not sure why we'd prefer this over `accuracy`
            - Doesn't work with multiclass data (though I think it's supposed to)
        - neg_mean_squared_error: Should take into account all labels simultaneously
            - Values are noisier than accuracy and difficult to interpret
        - neg_median_absolute_error: Same idea, but robust to outliers
            - No good for categorical data because it only comes out as whole numbers
        - r2: Not necessarily the square of R -- can be negative
            - Gives negative values on the test dataset
    - n_jobs: use this parameter here instead of in the model class (e.g. LogisticRegression)

